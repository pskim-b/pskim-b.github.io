<!doctype html>
<html lang="en-us">
  <head>
    <title>[Spark]  Apache Spark  클러스터 기본 아키텍쳐  - 기술 학습 블로그 // PSKim Blog</title>
    <link rel="shortcut icon" href="/favicon.ico" />
    <meta charset="utf-8" />
    <meta name="generator" content="Hugo 0.80.0" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="author" content="PS Kim" />
    <meta name="description" content="" />
    <link rel="stylesheet" href="http://localhost/css/main.min.6ec48a05dbb027adf559c3f069b5dac49d370d31894e1093e7b312399dd875f7.css" />

    <meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="[Spark]  Apache Spark  클러스터 기본 아키텍쳐 "/>
<meta name="twitter:description" content="🌟 spark
&#43;&#43;&#43;
Spark 특성  In-memory 클러스터 컴퓨팅 엔진이다. 별도의 프로그램 작성 없이 데이터를 병렬처리할 수있게 되고, 가용성을 보장해준다.  Spark &amp; its Features Apache Spark is an open source cluster computing framework for real-time data processing. The main feature of Apache Spark is its in-memory cluster computing that increases the processing speed of an application. Spark provides an interface for programming entire clusters with implicit *data parallelism and fault tolerance*."/>

    <meta property="og:title" content="[Spark]  Apache Spark  클러스터 기본 아키텍쳐 " />
<meta property="og:description" content="🌟 spark
&#43;&#43;&#43;
Spark 특성  In-memory 클러스터 컴퓨팅 엔진이다. 별도의 프로그램 작성 없이 데이터를 병렬처리할 수있게 되고, 가용성을 보장해준다.  Spark &amp; its Features Apache Spark is an open source cluster computing framework for real-time data processing. The main feature of Apache Spark is its in-memory cluster computing that increases the processing speed of an application. Spark provides an interface for programming entire clusters with implicit *data parallelism and fault tolerance*." />
<meta property="og:type" content="article" />
<meta property="og:url" content="http://localhost/posts/spark/20210413_spark_architecture/" />
<meta property="article:published_time" content="2021-04-09T00:00:00+00:00" />
<meta property="article:modified_time" content="2021-04-09T00:00:00+00:00" />

	
	
	<script async src="https://www.googletagmanager.com/gtag/js?id=G-TEFH3VCWGN"></script>
	
	<meta name="google-site-verification" content="u0ltI-GL7Num3qJUw3i_trrBYxESJVJTjI-k4GPWimU" />
	<script>
	window.dataLayer = window.dataLayer || [];
	function gtag(){dataLayer.push(arguments);}
	gtag('js', new Date());

	gtag('config', 'G-TEFH3VCWGN');
	</script>


  </head>
  <body>
    <header class="app-header">
      <a href="http://localhost"><img class="app-header-avatar" src="/avatar.jpg" alt="PS Kim" /></a>
      <h1>PSKim Blog</h1>
      <nav class="app-header-menu">
          <a class="app-header-menu-item" href="/">Home</a>
             - 
          
          <a class="app-header-menu-item" href="/tags/">Tags</a>
      </nav>
      <p>기술 학습 블로그</p>
      <div class="app-header-social">
        
          <a target="_blank" href="https://github.com/gohugoio" rel="noreferrer noopener"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-github">
  <title>github</title>
  <path d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37 0 0 0-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44 0 0 0 20 4.77 5.07 5.07 0 0 0 19.91 1S18.73.65 16 2.48a13.38 13.38 0 0 0-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07 0 0 0 5 4.77a5.44 5.44 0 0 0-1.5 3.78c0 5.42 3.3 6.61 6.44 7A3.37 3.37 0 0 0 9 18.13V22"></path>
</svg></a>
        
      </div>
    </header>
    <main class="app-container">
      
  <article class="post">
    <header class="post-header">
      <h1 class ="post-title">[Spark]  Apache Spark  클러스터 기본 아키텍쳐 </h1>
      <div class="post-meta">
        <div>
          <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-calendar">
  <title>calendar</title>
  <rect x="3" y="4" width="18" height="18" rx="2" ry="2"></rect><line x1="16" y1="2" x2="16" y2="6"></line><line x1="8" y1="2" x2="8" y2="6"></line><line x1="3" y1="10" x2="21" y2="10"></line>
</svg>
          Apr 9, 2021
        </div>
        <div>
          <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-clock">
  <title>clock</title>
  <circle cx="12" cy="12" r="10"></circle><polyline points="12 6 12 12 16 14"></polyline>
</svg>
          9 min read
        </div>
        <div>
          <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-tag">
  <title>tag</title>
  <path d="M20.59 13.41l-7.17 7.17a2 2 0 0 1-2.83 0L2 12V2h10l8.59 8.59a2 2 0 0 1 0 2.82z"></path><line x1="7" y1="7" x2="7" y2="7"></line>
</svg>
              <a class="tag" href="http://localhost/tags/spark/">spark</a>
              <a class="tag" href="http://localhost/tags/bigdata/">Bigdata</a>
        </div>
      </div>
    </header>
    <div class="post-content">
      <p>🌟 spark</p>
<p>+++</p>
<h2 id="spark-특성">Spark 특성</h2>
<ul>
<li>In-memory 클러스터 컴퓨팅 엔진이다.</li>
<li>별도의 프로그램 작성 없이 데이터를 병렬처리할 수있게 되고, 가용성을 보장해준다.</li>
</ul>
<h2 id="spark--its-features"><strong>Spark &amp; its Features</strong></h2>
<p>Apache Spark is an open source cluster computing framework for real-time data processing. The main feature of Apache Spark is its <em><strong>in-memory cluster computing</strong></em> that increases the processing speed of an application. Spark provides an interface for programming entire clusters with implicit *<strong>data parallelism and fault tolerance*</strong>. It is designed to cover a wide range of workloads such as batch applications, iterative algorithms, interactive queries, and streaming.</p>
<h3 id="features-of-apache-spark"><strong>Features of Apache Spark:</strong></h3>
<p><img src="https://www.edureka.co/blog/wp-content/uploads/2018/09/Picture5-2.png" alt="Spark Features- Spark Architecture-Edureka">                              Fig: Features of Spark</p>
<ol>
<li><strong>Speed</strong>**
**Spark runs up to 100 times faster than Hadoop MapReduce for large-scale data processing. It is also able to achieve this speed through controlled partitioning.</li>
<li>**Powerful Caching
**Simple programming layer provides powerful caching and disk persistence capabilities.</li>
<li>**Deployment
**It can be deployed through <em><strong>Mesos, Hadoop via YARN, or Spark’s own cluster manager.</strong></em></li>
<li><strong>Real-Time</strong>
It offers Real-time computation &amp; low latency because of <em><strong>in-memory computation.</strong></em></li>
<li><strong>Polyglot</strong>
Spark provides high-level APIs in Java, Scala, Python, and R. Spark code can be written in any of these four languages. It also provides a shell in Scala and Python.</li>
</ol>
<h2 id="spark-architecture-overview"><strong>Spark Architecture Overview</strong></h2>
<p>Apache Spark has a well-defined layered architecture where all the spark components and layers are loosely coupled. This architecture is further integrated with various extensions and libraries. Apache Spark Architecture is based on two main abstractions:</p>
<ul>
<li><em>Resilient Distributed Dataset (RDD)</em></li>
<li><em>Directed Acyclic Graph (DAG)</em></li>
</ul>
<p><img src="https://d1jnx9ba8s6j9r.cloudfront.net/blog/wp-content/uploads/2018/09/2018-09-28-18_12_51-Apache-Spark-Architecture-_-Understanding-the-Spark-Components-_-Edureka.png" alt="Spark Architecture _ Edureka">                            Fig: Spark Architecture</p>
<p>But before diving any deeper into the Spark architecture, let me explain few fundamental concepts of Spark like Spark Eco-system and RDD. This will help you in gaining better insights.</p>
<p>Let me first explain what is Spark Eco-System.</p>
<h2 id="spark-eco-system"><strong>Spark Eco-System</strong></h2>
<p>As you can see from the below image, the spark ecosystem is composed of various components like Spark SQL, Spark Streaming, MLlib, GraphX, and the Core API component.</p>
<p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAMLCwgAAACH5BAAAAAAALAAAAAABAAEAAAICRAEAOw==" alt="Spark Eco-system- Spark Architecture - edureka">                               Fig: Spark Eco-System</p>
<ol>
<li>**Spark Core
**Spark Core is the base engine for large-scale parallel and distributed data processing. Further, additional libraries which are built on the top of the core allows diverse workloads for streaming, SQL, and machine learning. It is responsible for memory management and fault recovery, scheduling, distributing and monitoring jobs on a cluster &amp; interacting with storage systems.</li>
<li>**Spark Streaming
**Spark Streaming is the component of Spark which is used to process real-time streaming data. Thus, it is a useful addition to the core Spark API. It enables high-throughput and fault-tolerant stream processing of live data streams.</li>
<li>**Spark SQL
**Spark SQL is a new module in Spark which integrates relational processing with Spark’s functional programming API. It supports querying data either via SQL or via the Hive Query Language. For those of you familiar with RDBMS, Spark SQL will be an easy transition from your earlier tools where you can extend the boundaries of traditional relational data processing.</li>
<li>**GraphX
**GraphX is the Spark API for graphs and graph-parallel computation. Thus, it extends the Spark RDD with a Resilient Distributed Property Graph. At a high-level, GraphX extends the Spark RDD abstraction by introducing the Resilient Distributed Property Graph (a directed multigraph with properties attached to each vertex and edge).</li>
<li><strong>MLlib (Machine Learning)</strong>
MLlib stands for Machine Learning Library. Spark MLlib is used to perform machine learning in Apache Spark.</li>
<li>***SparkR
***It is an R package that provides a distributed data frame implementation. It also supports operations like selection, filtering, aggregation but on large data-sets.</li>
</ol>
<p>As you can see, Spark comes packed with high-level libraries, including support for R, SQL, Python, Scala, Java etc. These standard libraries increase the seamless integrations in a complex workflow. Over this, it also allows various sets of services to integrate with it like MLlib, GraphX, SQL + Data Frames, Streaming services etc. to increase its capabilities.</p>
<p>Now, let’s discuss the fundamental Data Structure of Spark, i.e. RDD.</p>
<h4 id="subscribe-to-our-youtube-channel-to-get-new-updates">Subscribe to our YouTube channel to get new updates&hellip;</h4>
<iframe ng-non-bindable="" frameborder="0" hspace="0" marginheight="0" marginwidth="0" scrolling="no" tabindex="0" vspace="0" width="100%" id="I0_1618323628000" name="I0_1618323628000" src="https://www.youtube.com/subscribe_embed?usegapi=1&amp;channelid=UCkw4JCwteGrDHIsyIIKo4tQ&amp;layout=full&amp;count=default&amp;origin=https%3A%2F%2Fwww.edureka.co&amp;gsrc=3p&amp;ic=1&amp;jsh=m%3B%2F_%2Fscs%2Fapps-static%2F_%2Fjs%2Fk%3Doz.gapi.ko.47UJjQcsGLM.O%2Fam%3DAQ%2Fd%3D1%2Fct%3Dzgms%2Frs%3DAGLTcCNrQ1camlgvhO0NxcZ6L8-gYpjG0Q%2Fm%3D__features__#_methods=onPlusOne%2C_ready%2C_close%2C_open%2C_resizeMe%2C_renderstart%2Concircled%2Cdrefresh%2Cerefresh%2Conload&amp;id=I0_1618323628000&amp;_gfid=I0_1618323628000&amp;parent=https%3A%2F%2Fwww.edureka.co&amp;pfname=&amp;rpctoken=32770960" data-gapiattached="true" style="box-sizing: border-box; display: list-item; height: 48px; margin: 0px; text-align: center; max-width: 100%; min-width: 1px !important; z-index: 0 !important; position: static; top: 0px; width: 180px; border-style: none; left: 0px; visibility: visible;"></iframe>
<h2 id="resilient-distributed-datasetrdd"><strong>Resilient Distributed Dataset(RDD)</strong></h2>
<p>RDDs are the building blocks of any Spark application. RDDs Stands for:</p>
<ul>
<li>*<strong>Resilient:*</strong> Fault tolerant and is capable of rebuilding data on failure</li>
<li><em><strong>Distributed:</strong></em> Distributed data among the multiple nodes in a cluster</li>
<li>*<strong>Dataset:*</strong> Collection of partitioned data with values</li>
</ul>
<p><img src="https://d1jnx9ba8s6j9r.cloudfront.net/blog/wp-content/uploads/2018/07/Partitions.png" alt="Partitions - Spark Architecture - Edureka"></p>
<p>It is a layer of abstracted data over the distributed collection. It is immutable in nature and follows <em><a href="https://www.edureka.co/blog/spark-tutorial/#Spark_Features">lazy transformations</a></em>.</p>
<p>Now you might be wondering about its working. Well, the data in an RDD is split into chunks based on a key. RDDs are highly resilient, i.e, they are able to recover quickly from any issues as the same data chunks are replicated across multiple executor nodes. Thus, even if one executor node fails, another will still process the data. This allows you to perform your functional calculations against your dataset very quickly by harnessing the power of multiple nodes.</p>
<p><a href="https://www.edureka.co/apache-spark-scala-certification-training"><img src="https://d1jnx9ba8s6j9r.cloudfront.net/blog/wp-content/themes/edu-new/img/course-curriculum.png" alt="Course Curriculum">Apache Spark and Scala Certification TrainingInstructor-led SessionsReal-life Case StudiesAssessmentsLifetime AccessExplore Curriculum</a></p>
<p>Moreover, once you create an RDD it becomes *<strong>immutable*</strong>. By immutable I mean, an object whose state cannot be modified after it is created, but they can surely be transformed.</p>
<p>Talking about the distributed environment, each dataset in RDD is divided into logical partitions, which may be computed on different nodes of the cluster. Due to this, you can perform transformations or actions on the complete data parallelly. Also, you don’t have to worry about the distribution, because Spark takes care of that.</p>
<p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAMLCwgAAACH5BAAAAAAALAAAAAABAAEAAAICRAEAOw==" alt="Workflow- Spark Architecture - Edureka">                                  Workflow of RDD</p>
<p>There are two ways to create RDDs − parallelizing an existing collection in your driver program, or by referencing a dataset in an external storage system, such as a shared file system, HDFS, HBase, etc.</p>
<p>With RDDs, you can perform two types of operations:</p>
<ol>
<li><strong>Transformations:</strong> They are the operations that are applied to create a new RDD.</li>
<li><strong>Actions:</strong> They are applied on an RDD to instruct Apache Spark to apply computation and pass the result back to the driver.</li>
</ol>
<p>I hope you got a thorough understanding of RDD concepts. Now let’s move further and see the working of Spark Architecture.</p>
<h2 id="working-of-spark-architecture"><strong>Working of Spark Architecture</strong></h2>
<p>As you have already seen the basic architectural overview of Apache Spark, now let’s dive deeper into its working.</p>
<p>In your <strong>master node</strong>, you have the <em>driver program</em>, which drives your application. The code you are writing behaves as a driver program or if you are using the interactive shell, the shell acts as the driver program.</p>
<p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAMLCwgAAACH5BAAAAAAALAAAAAABAAEAAAICRAEAOw==" alt="Spark Architecture - Edureka">                 Fig: Spark Architecture</p>
<p>Inside the driver program, the first thing you do is, you <em>create</em> a <em><strong>Spark Context.</strong></em> Assume that the Spark context is a gateway to all the Spark functionalities. It is similar to your database connection. Any command you execute in your database goes through the database connection. Likewise, anything you do on Spark goes through Spark context.</p>
<p>Now, this Spark context works with the *<strong>cluster manager*</strong> to manage various jobs. The driver program &amp; Spark context takes care of the job execution within the cluster. A job is split into multiple tasks which are distributed over the worker node. Anytime an RDD is created in Spark context, it can be distributed across various nodes and can be cached there.</p>
<p><strong>W*orker nodes*</strong> are the slave nodes whose job is to basically execute the tasks. These tasks are then executed on the partitioned RDDs in the worker node and hence returns back the result to the Spark Context.</p>
<p>Spark Context takes the job, breaks the job in tasks and distribute them to the worker nodes. These tasks work on the partitioned RDD, perform operations, collect the results and return to the main Spark Context.</p>
<p>If you increase the number of workers, then you can divide jobs into more partitions and execute them parallelly over multiple systems. It will be a lot faster.</p>
<p>With the increase in the number of workers, memory size will also increase &amp; you can cache the jobs to execute it faster.</p>
<p>To know about the workflow of Spark Architecture, you can have a look at the <strong>infographic</strong> below:</p>
<h3 id="big-data-training">Big Data Training</h3>
<p><a href="https://www.edureka.co/big-data-hadoop-training-certification">BIG DATA HADOOP CERTIFICATION TRAININGBig Data Hadoop Certification Training<em>Reviews</em> 5(163009)</a></p>
<p><a href="https://www.edureka.co/pyspark-certification-training">PYTHON SPARK CERTIFICATION TRAINING USING PYSPARKPython Spark Certification Training using PySpark<em>Reviews</em> 5(6221)</a></p>
<p><a href="https://www.edureka.co/apache-spark-scala-certification-training">APACHE SPARK AND SCALA CERTIFICATION TRAININGApache Spark and Scala Certification Training<em>Reviews</em> 5(27411)</a></p>
<p><a href="https://www.edureka.co/splunk-certification-training">SPLUNK TRAINING &amp; CERTIFICATION- POWER USER &amp; ADMINSplunk Training &amp; Certification- Power User &amp; Admin<em>Reviews</em> 5(7892)</a></p>
<p><a href="https://www.edureka.co/kafka-certification-training">APACHE KAFKA CERTIFICATION TRAININGApache Kafka Certification Training<em>Reviews</em> 5(6549)</a></p>
<p><a href="https://www.edureka.co/elk-stack-training-sp">ELK STACK TRAINING &amp; CERTIFICATIONELK Stack Training &amp; Certification<em>Reviews</em> 5(1622)</a></p>
<p><a href="https://www.edureka.co/hadoop-administration-training-certification">HADOOP ADMINISTRATION CERTIFICATION TRAININGHadoop Administration Certification Training<em>Reviews</em> 5(25233)</a></p>
<p><a href="https://www.edureka.co/apache-storm-certification-training">APACHE STORM CERTIFICATION TRAININGApache Storm Certification Training<em>Reviews</em> 5(5551)</a></p>
<p><a href="https://www.edureka.co/comprehensive-hive">COMPREHENSIVE HIVE CERTIFICATION TRAININGComprehensive Hive Certification Training<em>Reviews</em> 5(2304)</a></p>
<p>Next</p>
<p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAMLCwgAAACH5BAAAAAAALAAAAAABAAEAAAICRAEAOw==" alt="Spark Architecture - Edureka">                             Fig: Spark Architecture Infographic</p>
<p><strong>STEP 1:</strong> The client submits spark user application code. When an application code is submitted, the driver implicitly converts user code that contains transformations and actions into a logically <em>directed acyclic graph</em> called <em><strong>DAG.</strong></em> At this stage, it also performs optimizations such as pipelining transformations.</p>
<p><strong>STEP 2:</strong> After that, it converts the logical graph called DAG into physical execution plan with many stages. After converting into a physical execution plan, it creates physical execution units called tasks under each stage. Then the tasks are bundled and sent to the cluster.</p>
<p><strong>STEP 3:</strong> Now the driver talks to the cluster manager and negotiates the resources. Cluster manager launches executors in worker nodes on behalf of the driver. At this point, the driver will send the tasks to the executors based on data placement. When executors start, they register themselves with drivers. So, the driver will have a complete view of executors that are executing the task.</p>
<p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAMLCwgAAACH5BAAAAAAALAAAAAABAAEAAAICRAEAOw==" alt="Spark Architecture - Edureka"></p>
<p><strong>STEP 4:</strong> During the course of execution of tasks, driver program will monitor the set of executors that runs. Driver node also schedules future tasks based on data placement.</p>
<p>This was all about Spark Architecture. Now, let’s get a hand’s on the working of a Spark shell.</p>
<h2 id="example-using-scala-in-spark-shell"><strong>Example using Scala in Spark shell</strong></h2>
<p>At first, let’s start the Spark shell by assuming that Hadoop and Spark daemons are up and running. *<strong>Web UI*</strong> port for Spark is *<strong>localhost:4040.*</strong></p>
<p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAMLCwgAAACH5BAAAAAAALAAAAAABAAEAAAICRAEAOw==" alt="Spark shell-Spark Architecture - Edureka">                                Fig: Spark-shell</p>
<p>Once you have started the Spark shell, now let’s see how to execute a word count example:</p>
<ol>
<li>In this case, I have created a simple text file and stored it in the hdfs directory. You can also use other large data files as well.
<img src="https://www.edureka.co/blog/wp-content/uploads/2018/09/2018-09-27-10_02_10-sample-file.PNG-%E2%80%8E-Photos.png" alt="img">                                         Fig: Input text file</li>
<li>Once the spark shell has started, let’s create an RDD. For this, you have to specify the input file path and apply the transformation <strong>flatMap()</strong>. Below code illustrates the same:</li>
</ol>
<pre><code>scala&gt; var map = sc.textFile(&quot;hdfs://localhost:9000/Example/sample.txt&quot;).flatMap(line =&gt; line.split(&quot; &quot;)).map(word =&gt; (word,1)); 
</code></pre><p>\3. On executing this code, an RDD will be created as shown in the figure.</p>
<p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAMLCwgAAACH5BAAAAAAALAAAAAABAAEAAAICRAEAOw==" alt="img">                                           Fig: RDD creation</p>
<p>\4. After that, you need to apply the action <strong>reduceByKey()</strong> to the created RDD.</p>
<pre><code>scala&gt; var counts = map.reduceByKey(_+_);
</code></pre><p>After applying action, execution starts as shown below.</p>
<p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAMLCwgAAACH5BAAAAAAALAAAAAABAAEAAAICRAEAOw==" alt="img">                                       Fig: Spark execution in the shell</p>
<p>\5. Next step is to save the output in a text file and specify the path to store the output.</p>
<p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAMLCwgAAACH5BAAAAAAALAAAAAABAAEAAAICRAEAOw==" alt="img">                        Fig: Specifying the Output path</p>
<p>\6. After specifying the output path, go to the <em>hdfs web browser <strong>localhost:50040.</strong></em> Here you can see the output text in the ‘part’ file as shown below.</p>
<p><img src="https://www.edureka.co/blog/wp-content/uploads/2018/09/2018-09-27-09_55_11-PART-FILE-11.PNG-%E2%80%8E-Photos.png" alt="img">                                     Fig: Output part file</p>
<p>​    \7. Below figure shows the output text present in the ‘part’ file.</p>
<p><img src="https://www.edureka.co/blog/wp-content/uploads/2018/09/2018-09-27-09_56_33-part-file.PNG-%E2%80%8E-Photos.png" alt="img">                 Fig: Output text</p>
<p>I hope that you have understood how to create a Spark Application and arrive at the output.ur Newsletter, and get personalized recommendations.</p>

    </div>
    <div class="post-footer">
      
    </div>
  </article>

    </main>
  </body>
</html>
