<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on PSKim Blog</title>
    <link>https://pskim-b.github.io/posts/</link>
    <description>Recent content in Posts on PSKim Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <lastBuildDate>Sun, 21 Mar 2021 00:00:00 +0000</lastBuildDate><atom:link href="https://pskim-b.github.io/posts/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>[Enterprise Big DataLake] 01. Datalake 소개 </title>
      <link>https://pskim-b.github.io/posts/datalake/ebdl_01_datalake01/</link>
      <pubDate>Sun, 21 Mar 2021 00:00:00 +0000</pubDate>
      
      <guid>https://pskim-b.github.io/posts/datalake/ebdl_01_datalake01/</guid>
      <description>💾
국내에도 1~2년부터 데이터 레이크(Datalake)를 도입해야 한다고 하는 이야기가 우후죽순과도 같이 자라났다. 기존에 하둡도 있었고 Isilon 같은 스토리지도 있었지만, 데이터 레이크 도입에 대한 이야기는 AWS S3가 범용 스토리지로 사용되면서 더욱 활발해 지지 않았나 생각된다. 하지만 실제로 데이터 레이크를 이야기에 대한 이야기를 나누어보면 모두 생각 하는바가 달랐다. 단순히 더 큰 규모의 DW를 이야기하는가 하면, 또 다른 종류의 빅데이터 저장소(하둡과도 같은)를 이야기하기도 했다.
업무적으로 데이터 레이크를 관리하기 위한 도구를 기획하고 개발하는 업무를 하고 있음에도 불구하고, 이해관계자들의 청사진과 요구사항이 모두 달라 중심을 잡고 목소리를 내기 힘들어지기 시작했을 무렵 IT도서의 바이블과도 같은 O&amp;rsquo;REILLY의 도서로 &amp;lsquo;The Enterprise Big Data Lake&amp;rsquo;가 이미 번역되어 나온 것을 알게되어 일단 구매 후 읽어보기로 했다.</description>
    </item>
    
    <item>
      <title>[ADsP] 데이터분석 주제 정리 </title>
      <link>https://pskim-b.github.io/posts/adsp/99_adsp_subject_map/</link>
      <pubDate>Sun, 07 Mar 2021 00:00:00 +0000</pubDate>
      
      <guid>https://pskim-b.github.io/posts/adsp/99_adsp_subject_map/</guid>
      <description>😃
데이터 분석 과목에서 다루는 주제들이 생각보다 넓다보니 한눈에 보기가 쉽지 않다. 각 기술들에 상세한 분석을 하기전에 전체적인 흐름을 정리하고자 한다.
   R 기초
  통계분석
  통계분석 개요 (모집단, 표본, 표본추출방법)
 표본추출방법  확률적추출 (단순 무작위 추출, 계통추출, 층화 추출, 군집 추출) 비확률적 추출 (판단추출, 할당추출, 편의추출)   자료의 종류 (명목척도, 서열척도, 등간척도, 비율척도)    통계분석 분류
 기술통계 / 추론통계 모수통계 / 비모수통계    확률 / 확률분포 (확률곱, 조건부 확률,</description>
    </item>
    
    <item>
      <title>[QnA-Hadoop] 01.Hadoop QnA </title>
      <link>https://pskim-b.github.io/posts/hadoop/qna_01_hadoopqna/</link>
      <pubDate>Sun, 07 Mar 2021 00:00:00 +0000</pubDate>
      
      <guid>https://pskim-b.github.io/posts/hadoop/qna_01_hadoopqna/</guid>
      <description>😃
데이터 분석 과목에서 다루는 주제들이 생각보다 넓다보니 한눈에 보기가 쉽지 않다. 각 기술들에 상세한 분석을 하기전에 전체적인 흐름을 정리하고자 한다.
  R 기초   </description>
    </item>
    
    <item>
      <title>[ADsP]5.R 데이터 프레임 활용 </title>
      <link>https://pskim-b.github.io/posts/adsp/5_r_data_frame/</link>
      <pubDate>Tue, 16 Feb 2021 00:00:00 +0000</pubDate>
      
      <guid>https://pskim-b.github.io/posts/adsp/5_r_data_frame/</guid>
      <description>😃
데이터 프레임의 데이터를 행/열 단위로 추출/제거/수정 함으로써 데이터 분석을 위한 데이터 클랜징 작업을 수행할 수 있다. 많은 경우 미리 데이터가 클랜징되어 들어오지만, 만약 R을 통해서 데이터를 수정해야 하는 경우 데이터 프레임을 사용하면 좋다.
 데이터 프레임 생성 # 벡터를 통한 생성  &amp;gt; v1=c(1,2,3,4) &amp;gt; v2=c(&amp;#34;a&amp;#34;,&amp;#34;b&amp;#34;,&amp;#34;c&amp;#34;,&amp;#34;d&amp;#34;) &amp;gt; v3=c(T,F,F,T) &amp;gt; data.frame(v1,v2,v3) v1 v2 v3 1 1 a TRUE 2 2 b FALSE 3 3 c FALSE 4 4 d TRUE # 행렬을 통한 생성  &amp;gt; m1 = matrix(c(1,3,5,7,9,2,4,6,8,19,2,3,5,7,11),ncol=3) &amp;gt; as.</description>
    </item>
    
    <item>
      <title>[ADsP]4.R 데이터 구조 ( vector , list, dataframe )</title>
      <link>https://pskim-b.github.io/posts/adsp/4_r_data_structure/</link>
      <pubDate>Tue, 02 Feb 2021 00:00:00 +0000</pubDate>
      
      <guid>https://pskim-b.github.io/posts/adsp/4_r_data_structure/</guid>
      <description>😃
R에서 간단히 파일을 입출력하는 방법을 알아보았다. 이제 그 데이터를 가지고 분석하기 위해서 내부적으로 R의 데이터 구조로 저장해야 한다. R에는 크게 벡터(vector), 리스트(list), 행렬(matrix), 배열(Arrays), 요인(Factors), 데이터프레임(Dataframe), 스칼라(scala) 로 나눌 수 있다.
 R 데이터 구조 1. 스칼라(scala) 단일 값을 갖는 자료구조로 프로그램 내에서 원소가 하나인 벡터처럼 인식된다. logical, integer, double, complex, character 와 같은 데이터 뿐만 아니라 NULL, NA(Not Available)도 포함한 모든 단일 값을 이야기한다.
&amp;gt; pi [1] 3.141593 &amp;gt; mode(pi) [1] &amp;#34;numaric&amp;#34; 2.</description>
    </item>
    
    <item>
      <title>[ADsP]3.R 데이터 처리/분석</title>
      <link>https://pskim-b.github.io/posts/adsp/3_r_data_processing/</link>
      <pubDate>Sun, 31 Jan 2021 00:00:00 +0000</pubDate>
      
      <guid>https://pskim-b.github.io/posts/adsp/3_r_data_processing/</guid>
      <description>😃
지난 글에서 간단하게 R의 기본언어를 알아보았다. 이번 글에서는 실제로 R을 사용하여 데이터를 입출력하는 방법을 알아보려고 한다.
 데이터 처리/분석 과정 모든 일이 그러하듯이 데이터 분석을 시작할 때에도 그것의 목적을 명확히 하고 이에 맞는 분석 방법론을 선택해야 한다. 정확한 분석을 위해서는 분석가가 설계한대로 올바른 데이터가 입력되어야 한다. 따라서 데이터를 분석이 가능한 형태로 전처리하는 과정이 필요하며 이를 데이터 핸들링이라고도 한다. 그리고 데이터 분석 이후에 의사결정권자와 고객에 보고서 형태로 제공됨으로써 분석과제가 종료된다.</description>
    </item>
    
    <item>
      <title>[ADsP]2.R 프로그램 기초</title>
      <link>https://pskim-b.github.io/posts/adsp/2_r_programming/</link>
      <pubDate>Tue, 26 Jan 2021 00:00:00 +0000</pubDate>
      
      <guid>https://pskim-b.github.io/posts/adsp/2_r_programming/</guid>
      <description>😃
요즘의 대세는 python이야!! 라고 많은 이들이 외치지만 전통적인 분석 언어 R도 많이 사용된다. 분석가에 따라서 R을 쓰기도하고 python을 쓰기도 해서, 많은 프로젝트들에서 R과 python 모두를 만족하는 환경을 제공하고 있다(일반적으로는 sparkR/pySpark로 분산환경에서 학습/분석작업을 수행하고 있다). 이번 장에는 간단히 R로 분석을 하기 위한 간단한 문법을 알아보자!!
 R소개 R은 오픈소스 프로그램으로 통계 및 데이터마이닝과 그 결과를 그래프를 위한 언어이다. 오픈소스이기 때문에 지속적으로 최신의 통계분석과 데이터 마이닝 기술이 반영되어 발전되고 있다. 새로운 것을 이해하는데 가장 좋은 방법 중 하나는 기존에 친숙한 도구들과 비교하는 것이다.</description>
    </item>
    
    <item>
      <title>[ADsP]0.분석(준)전문가 자격증 공부해보자</title>
      <link>https://pskim-b.github.io/posts/adsp/0_start/</link>
      <pubDate>Mon, 25 Jan 2021 00:00:00 +0000</pubDate>
      
      <guid>https://pskim-b.github.io/posts/adsp/0_start/</guid>
      <description>😃
빅데이터를 처리하는 솔루션을 개발하고 구축하는게 나의 주된 업무이지만, 기술에 대한 호기심이 많은터라 새로운 분야를 접할 때마다 조금조금씩 공부하고는 한다. AI/분석업무도 나에게는 그런 영역 중 하나인데, 대학교때 공통교양이였던 통계학을 멀리해서 벌을 받는 것인지 몰라도 기억속에 그리 오래 남지 않았다.
그래서 알아두면 나쁘지 않을 생각을 하고 있는 중에 요즘 데이터 관련 직종으로 취업하는 대학생이라면 하나씩 다 가지고 있다던 데이터분석 준전문가(ADsP) 자격증을 따보기로 결심했다. 자격증 자체는 &amp;lsquo;정보처리기사&amp;rsquo; 같이 크게 의미없는 한줄이 될 수 있겠다만, 그냥 재미있을 것 같으니까 해보기로 한다.</description>
    </item>
    
    <item>
      <title>[ADsP]1.통계분석의 시작</title>
      <link>https://pskim-b.github.io/posts/adsp/1_statistical_analysis/</link>
      <pubDate>Mon, 25 Jan 2021 00:00:00 +0000</pubDate>
      
      <guid>https://pskim-b.github.io/posts/adsp/1_statistical_analysis/</guid>
      <description>😃
이상한 순서이지만, 3장의 &amp;lsquo;데이터 분석&amp;rsquo; 을 먼저 보려고 한다. 그 이유는 1장의 &amp;lsquo;데이터의 이해&amp;rsquo; 부터 보게된다면 뒤로 갈 수록 쳐질것 같은 기분이 들어서이다. 두렵고 어려울 것이라 생각되는 부분을 가장 먼저 부수고 들어가보자!!
 내용 일반적으로 데이터를 처리할 때 legacy시스템에서 직접 데이터를 가져오기도 하지만 ODS(Operational Data Store) 에서 데이터를 가져오는 것을 권고한다. 그 이유는 운영 중에 legacy시스템에서 데이터를 직접 가져오는 것은 legacy시스템에 부하를 유발함으로 운영 서비스에 영향을 끼칠 수 있기 때문이다.</description>
    </item>
    
    <item>
      <title>Windows10 Git &#39;OpenSSH Key is invalid&#39; 오류 </title>
      <link>https://pskim-b.github.io/posts/etc/20210122_git_win10_auth/</link>
      <pubDate>Fri, 22 Jan 2021 22:42:30 +0900</pubDate>
      
      <guid>https://pskim-b.github.io/posts/etc/20210122_git_win10_auth/</guid>
      <description>Windows10 환경에서 생성한 rsa key가 Github account/repository에 SSH키를 등록이 안되는 경우가 있다.
다른 블로그 및 stackoverflow에서도 아래와 같이 rsa key 생성을 가이드 하고 있다.
PS&amp;gt; ssh-keygen -b 2048 -t rsa 너무 간단한 작업이여서 다를 게 없을 것 같은데 안되서 굉장히 당황스러웠다.
위와 같은 명령어로 생성된 ~/.ssh/id_rsa.pub 파일의 내용을 github에 등록하면 &amp;lsquo;Key is invalid. You must supply a key in OpenSSH public key format&amp;rsquo; 와 같은 에러메세지가 나온다.
왜 이상한 포맷으로 생성되는지 잘 모르겠으나, 이러한 경우 아래와 같이 rsa 대신 ed25519를 생성하여 id_25519.</description>
    </item>
    
    <item>
      <title>[HIVE] 파티션 타입 및 종류</title>
      <link>https://pskim-b.github.io/posts/hive/hive_partition_type/</link>
      <pubDate>Tue, 28 Aug 2018 00:00:00 +0000</pubDate>
      
      <guid>https://pskim-b.github.io/posts/hive/hive_partition_type/</guid>
      <description>😃
Hive 파티션(partition)의 개념은 RDBMS 와 크게 다르지 않다. 테이블을 하나 이상의 키로 파티셔닝(partitioning) 할 수 있다. 기본적으로 테이블 생성 시 DDL문을 통해 파티션키 유무를 정할 수 있지만, row가 많은 Fact 테이블 같은 경우는 선택이 아닌 필수이다. 데이터를 조회 할 때 파티션 키 값을 잘 구성해야 hive 내부적으로 skip-scan이 발생하여 불필요한 I/O를 최소화 할 수 있다. 또한 파티션 키의 순서에 따라 hdfs 상의 디렉토리 구조가 결정됨으로 워크로드에 따라 그 순서도 적절히 결정해야 한다.</description>
    </item>
    
  </channel>
</rss>
